# -*- coding: utf-8 -*-
"""Projektmunka.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QTIEeb8wDTJ2Tj0KDcMUB2h8f3-U5UIS

#Importok
"""

!pip install bitstring
from google.colab import drive
import numpy as np
import pandas as pd
from PIL import Image, ImageEnhance
from PIL import ImageOps, ImageMath
from matplotlib import pyplot as plt
import cv2
import pylab
import struct
import bitstring
from PIL import Image
import tensorflow as tf
import sys
import codecs
from google.colab import files

"""#Adatbázis importálása"""

drive.mount('/content/gdrive')

filename = "/content/gdrive/My Drive/ETL4/ETL4C"

"""#Alap változók"""

hiragana_chars = "をゐゑあいうえおかきくけこさしすせそたちつてとなにぬねのはひふへほまみむめもやゆよらりるれろわん"
latin = "wo,wi,we,a,i,u,e,o,ka,ki,ku,ke,ko,sa,shi,su,se,so,ta,chi,tsu,te,to,na,ni,nu,ne,no,ha,hi,fu,he,ho,ma,mi,mu,me,mo,ya,yu,yo,ra,ri,ru,re,ro,wa,n";



def read_input(f, pos=2):
   f = bitstring.ConstBitStream(filename=f)
   f.bytepos = pos * 2952
   r = f.readlist('2*uint:36,uint:8,pad:28,uint:8,pad:28,4*uint:6,pad:12,15*uint:36,pad:1008,bytes:21888')
   return r

def create_data():
    data =  np.zeros((6113,76,72)) #5472
    labels = np.zeros(6113)
    for i in range(6113):
        r = read_input(filename,pos=i)
        iF = Image.frombytes('F', (r[18], r[19]), r[-1], 'bit', 4)
        iP = iF.convert('L')
        enhancer = ImageEnhance.Brightness(iP)
        iE = enhancer.enhance(r[20])
        temp=np.array(iE)
        data[i,:,:] = temp
        labels[i] = r[2]
    return (data,labels)

def preprocessing_data(data1):
    kernel = np.ones((3,3),np.float32)/9
    crop_temp = np.zeros((data.shape[0],data1.shape[2],data1.shape[2]))
    for i in range(data1.shape[0]):
          dst = cv2.GaussianBlur(data1[i,:,:],(3,3),0)
          ret,data1[i,:,:] = cv2.threshold(dst,0,255,cv2.THRESH_TOZERO+cv2.THRESH_OTSU)
          crop_temp[i,:,:] = data1[i,:72,:]
    return crop_temp

"""#Képnormalizálás"""

r = read_input(filename)
(data,labels) = create_data()
data = np.array(data, dtype = np.uint8) 

data1=data.copy()
data1 = preprocessing_data(data1)

#data1 labels

#print(data[1].shape)

for i in range(len(labels)):
  if(int(labels[i]) == 166):
    labels[i] = 0
  elif(int(labels[i]) == 168):
    labels[i] = 1
  elif(int(labels[i]) == 170):
    labels[i] = 2
  else:
    labels[i] = labels[i] - 174

for i in range(5):
  img = Image.fromarray(np.uint8(data1[i] * 255) , 'L')
  print(hiragana_chars[int(labels[i])])
  display(img)

print(data1.shape)

"""#Adatfeldolgozás"""

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
def unison_shuffled_copies(a, b):
    assert len(a) == len(b)
    p = np.random.permutation(len(a))
    return a[p], b[p]

data1 = data1/160

dataset_size = len(data1)
#data1 = data1.reshape(dataset_size,-1)

c_features, data_labels_encoded = unison_shuffled_copies(data1, labels)

data_labels_encoded_ohe = np.reshape(data_labels_encoded,(-1,1))

ohe_enc = OneHotEncoder()
ohe_enc.fit(data_labels_encoded_ohe)
data_labels_encoded_ohe = ohe_enc.transform(data_labels_encoded_ohe).toarray()

# Osztályok száma
num_classes = data_labels_encoded_ohe.shape[1]

#print(data_labels_encoded)

distrib = {}


for i in range(len(labels)):
  try:
    distrib[int(labels[i])] += 1
  except:
    distrib[int(labels[i])] = 1

plt.scatter(distrib.keys(),distrib.values(),vmin=0)

# Adatok szétválasztása

tr_data = c_features[:int(c_features.shape[0]*0.8),:]
tr_labels_ohe = data_labels_encoded_ohe[:int(data_labels_encoded_ohe.shape[0]*0.8),:]
tr_labels = data_labels_encoded[:int(data_labels_encoded_ohe.shape[0]*0.8)]

val_data = c_features[int(c_features.shape[0]*0.8):int(c_features.shape[0]*0.9),:]
val_labels_ohe = data_labels_encoded_ohe[int(data_labels_encoded_ohe.shape[0]*0.8):int(data_labels_encoded_ohe.shape[0]*0.9),:]
val_labels = data_labels_encoded[int(data_labels_encoded.shape[0]*0.8):int(data_labels_encoded.shape[0]*0.9)]

test_data = c_features[int(data_labels_encoded_ohe.shape[0]*0.9):,:]
test_labels_ohe = data_labels_encoded_ohe[int(data_labels_encoded_ohe.shape[0]*0.9):,:]
test_labels = data_labels_encoded[int(data_labels_encoded.shape[0]*0.9):]

import math
graph_labels = [r'Train (' + str(int(math.ceil(len(tr_data)/len(data1) * 100))) + '%)', r'Validation (' + str(int(math.ceil(len(val_data)/len(data1) * 100))) + '%)', r'Test (' + str(int(math.floor((len(test_data)/len(data1) * 100)))) + '%)']
sizes = [len(tr_data)/len(data1)*100,len(val_data)/len(data1)*100,len(test_data)/len(data1)*100]
colors = ['yellowgreen', 'gold', 'lightskyblue']
patches, texts = plt.pie(sizes, colors=colors, startangle=90)
plt.legend(patches, graph_labels, loc="best")

plt.axis('equal')
plt.tight_layout()
plt.show()

"""#Hagyományos SVM háló"""

from sklearn import svm
from sklearn.model_selection import GridSearchCV
#clf = svm.SVC(gamma=0.1, C=10)

parameters = {'kernel':('linear', 'rbf'), 'C':(0.1, 1, 10)}
clf = GridSearchCV(svm.SVC(degree=4), parameters)
#clf = svm.SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,
    #decision_function_shape='ovr', degree=4, gamma='auto_deprecated',
    #kernel='linear', max_iter=-1, probability=False, random_state=None,
    #shrinking=True, tol=0.001, verbose=False) #legjobb model
#clf.fit(x_train, y_train_cat)
#print(clf.best_score_) 
#print(clf.best_params_)
#model = clf.best_estimator_

#print(model)

#train = clf.fit(x_train, y_train_cat)
#train

flattened_tr_data = tr_data.reshape(len(tr_data),-1)
flattened_val_data = val_data.reshape(len(val_data),-1)
flattened_test_data = test_data.reshape(len(test_data),-1)

clf = svm.SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=4, gamma='auto_deprecated',
    kernel='linear', max_iter=-1, probability=False, random_state=None,
    shrinking=True, tol=0.001, verbose=False) #legjobb model,
train = clf.fit(flattened_tr_data,tr_labels)
train

# Evaluate performance

# Train set
train_pred = clf.predict(flattened_tr_data)
train_pred_scores = train_pred == tr_labels
train_pred_scores = sum(train_pred_scores)
print("Train score: ", (train_pred_scores/flattened_tr_data.shape[0])*100)

# Validation set
val_pred = clf.predict(flattened_val_data)
val_pred_scores = val_pred == val_labels
val_pred_scores = sum(val_pred_scores)
print("Val score: ", (val_pred_scores/flattened_val_data.shape[0])*100)

# Test set
test_pred = clf.predict(flattened_test_data)
test_pred_scores = test_pred == test_labels
test_pred_scores = sum(test_pred_scores)
print("Test score: ", (test_pred_scores/flattened_test_data.shape[0])*100)

print(test_pred[1])
abc = test_data.reshape(len(test_data),72,72)
for i in range(5):
  img = Image.fromarray(np.uint8(abc[i] * 255) , 'L')
  display(img)
  print("Guess: " + hiragana_chars[int(test_pred[i])] + " Correct: " + hiragana_chars[int(test_labels[i])])

abc = val_data.reshape(len(val_data),72,72)
for i in range(5):
  img = Image.fromarray(np.uint8(abc[i] * 255) , 'L')
  display(img)
  print("Guess: " + hiragana_chars[int(val_pred[i])] + " Correct: " + hiragana_chars[int(val_labels[i])])

"""#TF Mélyháló"""

learning_rate = 0.2
num_steps = 1000
batch_size = 256
display_step = 100

n_hidden_1 = 512
n_hidden_2 = 512
n_hidden_3 = 256
num_input = 5184
num_classes = 48

print(tr_data.shape)

tr_data_flattened = tr_data.reshape(len(tr_data),-1)

print(tr_data_flattened[0].shape)

input_fn = tf.estimator.inputs.numpy_input_fn(
    x={'images': tr_data_flattened}, y = tr_labels,batch_size = batch_size,num_epochs = None,shuffle=True)

def neural_net(x_dict):
  x = x_dict['images']

  layer_1 = tf.layers.dense(x,n_hidden_1,activation=tf.nn.relu)

  layer_2 = tf.layers.dense(layer_1, n_hidden_2, activation=tf.nn.relu)

  out_layer = tf.layers.dense(layer_2,num_classes)
  return out_layer

def model_fn(features,labels,mode):
  logits = neural_net_dowd(features)

  pred_classes = tf.argmax(logits,axis=1)
  pred_probas = tf.nn.softmax(logits)

  if mode == tf.estimator.ModeKeys.PREDICT:
    return tf.estimator.EstimatorSpec(mode,predictions=pred_classes)

  loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
        logits=logits, labels=tf.cast(labels, dtype=tf.int32)))
  optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
  train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())

  acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)

  estim_specs = tf.estimator.EstimatorSpec(
      mode=mode,
      predictions=pred_classes,
      loss=loss_op,
      train_op=train_op,
      eval_metric_ops={'accuracy': acc_op})

  return estim_specs

model = tf.estimator.Estimator(model_fn)

model.train(input_fn, steps=2000)

test_data_flattened = test_data.reshape(len(test_data),-1)
val_data_flattened = val_data.reshape(len(val_data),-1)

run_config = tf.estimator.RunConfig(save_checkpoints_steps=1000)

input_fn_test = tf.estimator.inputs.numpy_input_fn(
    x={'images':test_data_flattened}, y = test_labels,
    batch_size= batch_size,shuffle=False)
input_fn_val = tf.estimator.inputs.numpy_input_fn(
    x={'images':val_data_flattened}, y = val_labels,
    batch_size= batch_size,shuffle=False)

predictions = model.predict(input_fn_val)

incorrect = 0
allData = len(val_data_flattened)
k = 0
for i in predictions:
  if(i != val_labels[k]):
    incorrect += 1
  k+=1
print(((allData-incorrect)/allData) * 100)

dropout = 1.0
def neural_net_do(x_dict):
  x = x_dict['images']

  layer_1 = tf.layers.dense(x,n_hidden_1,activation=tf.nn.relu)
  layer_1 = tf.layers.dropout(layer_1,dropout)
  layer_2 = tf.layers.dense(layer_1, n_hidden_2, activation=tf.nn.relu)
  layer_2 = tf.layers.dropout(layer_2,dropout)
  layer_3 = tf.layers.dense(layer_2,n_hidden_3,activation=tf.nn.relu)
  out_layer = tf.layers.dense(layer_3,num_classes)
  return out_layer

regularizer = tf.contrib.layers.l1_regularizer(0.5)
def neural_net_wd(x_dict):
  x = x_dict['images']

  layer_1 = tf.layers.dense(x,n_hidden_1,activation=tf.nn.relu,kernel_regularizer=regularizer)

  layer_2 = tf.layers.dense(layer_1, n_hidden_2, activation=tf.nn.relu,kernel_regularizer=regularizer)

  out_layer = tf.layers.dense(layer_2,num_classes,kernel_regularizer=regularizer)
  return out_layer

dropout = 0
regularizer = tf.contrib.layers.l1_regularizer(0.5)
def neural_net_dowd(x_dict):
  x = x_dict['images']

  layer_1 = tf.layers.dense(x,n_hidden_1,activation=tf.nn.relu,kernel_regularizer=regularizer)
  layer_1 = tf.layers.dropout(layer_1,dropout)
  layer_2 = tf.layers.dense(layer_1, n_hidden_2, activation=tf.nn.relu,kernel_regularizer=regularizer)
  layer_2 = tf.layers.dropout(layer_2,dropout)
  layer_3 = tf.layers.dense(layer_2,n_hidden_3,activation=tf.nn.relu,kernel_regularizer=regularizer)

  out_layer = tf.layers.dense(layer_3,num_classes,kernel_regularizer=regularizer)
  return out_layer

results = np.zeros((5,5))

model.train(input_fn, steps=num_steps)
result = model.evaluate(input_fn_test)
results[4][4] = result['accuracy']

print(results)
epochs = [1,2,3,4,5]

for i in range(5):
  plt.plot(epochs,results[i], label = "Dropout: " + str((i * 2 + 2)/10))

plt.xlabel('Epochs') 
plt.ylabel('Accuracy') 
plt.title('Dropout') 
plt.legend() 
plt.show()

"""#Konvulúciós háló"""

import keras

tr_data_conv = np.expand_dims(tr_data,axis=3) #28,28,1
val_data_conv = np.expand_dims(val_data,axis=3)
test_data_conv = np.expand_dims(test_data,axis=3)

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten,Dropout
from keras import regularizers
from keras.callbacks import ModelCheckpoint


def create_model():
  num_filters = 64
  filter_size = 6
  pool_size = 3
  model = Sequential()
  model.add(Conv2D(num_filters,
          filter_size,
          input_shape=(72, 72, 1),
          strides=1,
          padding='same',
          activation='relu'))
  model.add(Dense(128, activation='relu'))
  model.add(Conv2D(num_filters,
          filter_size,
          input_shape=(36, 36, 1),
          strides=1,
          padding='same',
          activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(MaxPooling2D(pool_size=pool_size,padding='valid'))
  model.add(Dropout(0.4))
  model.add(Dense(32, activation='relu'))
  model.add(Conv2D(num_filters,
          filter_size,
          input_shape=(17, 17, 1),
          strides=1,
          padding='same',
          activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(MaxPooling2D(pool_size=pool_size))
  model.add(Flatten())
  model.add(Dense(128, activation='relu'))
  model.add(Dense(48, activation='softmax'))

  
  return model

def train_model():
  model = create_model()

  filepath="weights-improvement-{epoch:02d}-{val_acc:02d}.hdf5"
  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
  callbacks_list = [checkpoint]
  model.compile(
    'adam',
    loss='categorical_crossentropy',
    metrics=['accuracy'],
  )

  metrics  = model.fit(
    tr_data_conv,
    tr_labels_ohe,
    epochs=20,
    validation_data=(test_data_conv,test_labels_ohe),
    callbacks=callbacks_list
  )

model.summary()

model.load_weights('/content/weights-improvement-15-val_acc91.hdf5')

predictions = model.predict(test_data_conv)
incorrect = 0
for i in range(len(predictions)):
  if(np.argmax(predictions[i]) != np.argmax(test_labels_ohe[i])):
    incorrect += 1
print("Accuracy: " + str( (round((len(predictions)-incorrect) / len(predictions)*100,2) )) + "%")

for i in range(len(predictions)):
  if(np.argmax(predictions[i]) != np.argmax(test_labels_ohe[i])):
    abc = test_data.reshape(len(test_data),72,72)
    img = Image.fromarray(np.uint8(abc[i] * 255) , 'L')
    display(img)
    print("Guess: " +  hiragana_chars[int(np.argmax(predictions[i]))] + "Correct: " + hiragana_chars[int(np.argmax(test_labels_ohe[i]))])

plt.plot(metrics.history['acc'])
plt.plot(metrics.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'validation accuracy'], loc='center right')
plt.show()
plt.plot(metrics.history['loss'])
plt.plot(metrics.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'validation loss'], loc='center right')
plt.show()

#Save model
model.save("bestResult.h5")